,completeness,test_validity,generation_time,memory_usage,llm_uc_score,llm_tc_score,perplexity,coherence,readability,num_runs,composite_score,rank
stablelm_3b,1.0,1.0,82.68302655220032,47.37890625,10.0,8.0,24.146965980529785,0.21563675999641418,57.496941817734054,1,0.8302490043121163,1.0
phi2,1.0,1.0,426.53199577331543,9711.4609375,10.0,7.699999999999999,10.092716217041016,0.1762961447238922,59.75404250792916,1,0.7336112150125114,6.0
opt_1.3b,1.0,1.0,65.1174898147583,71.09765625,3.6,7.8,63.82966423034668,1.0,38.32500000000003,1,0.649467589255244,9.0
tinyllama,1.0,1.0,88.35208320617676,1421.3984375,8.3,7.0,6.5122233629226685,0.28437355160713196,58.996168620463564,1,0.8091004701275042,3.0
pythia_1b,1.0,1.0,73.51725196838379,23.3203125,3.6,4.1,63.82966423034668,1.0,38.32500000000003,1,0.5517710134363265,10.0
Qwen2-7B-Instruct,1.0,1.0,1023.9494504928589,-65.19921875,10.0,7.6,23.311321258544922,0.1837809681892395,39.92725521052401,1,0.6964867133934839,8.0
gemma_7b_it_4bit,1.0,1.0,1295.886352777481,-185.45703125,10.0,8.3,22.723668098449707,0.17395344376564026,61.75384814426563,1,0.8085672273012809,4.0
mistral,1.0,1.0,1333.8966746330261,897.47265625,10.0,8.2,21.733774185180664,0.18519407510757446,45.923411228457354,1,0.7206494604651822,7.0
Meta-Llama-3-8B-Instruct,1.0,1.0,761.4354326725006,-286.26171875,10.0,8.0,22.99743938446045,0.15398064255714417,61.36105978260872,1,0.8186194114547768,2.0
Falcon3-7B-Base,1.0,1.0,2778.5484154224396,-129.08203125,9.7,7.699999999999999,9.2964608669281,0.1666388362646103,61.15464761575937,1,0.7483730324902756,5.0
